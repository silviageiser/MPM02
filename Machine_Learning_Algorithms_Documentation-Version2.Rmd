---
title: "Machine_Learning_Algorithms_Documentation"
author: "Geiser, Gruen, Hefti, Kuster"
date: "07/01/2021"
output:
  html_document:
    toc: yes
    toc_depth: 2
    number_sections: yes
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '2'
---



# Machine Learning Algorithms

## Dataset - German Housing Data

We cleaned the origin data set: Variable rooms, bathrooms, bedrooms, floors, garages, Year_built, Year_renovated changed from decimal to integer.

We created a subset with buildings up to 20 rooms for our analysis.

**Load of the cleaned dataset**

```{r}
data <- read.csv('german_housing_cleaned.csv',header =T, encoding='UTF-8')
head(data)
```
# Analysis of variables

**Inspection of all variables**
```{r}
str(data)
summary(data)
colnames(data)
```


Analysis of the distribution of the variables: 'Price', 'Living_space', 'Rooms' and 'Lot'
```{r}
par(mfrow = c(2,2))

#Price
plot(density(data$Price))

#Living_space
plot(density(data$Living_space))

#Rooms
hist(data$Rooms)

#Lot
plot(density(data$Lot))



```
Result: The variables are right skewed


Therefore we will use the Log Transformation of the variables 'Price', 'Living_space', 'Rooms', 'Lot' to get a nearly normal distribution
``` {r}
par(mfrow = c(2,2))

#Price
price.log <- density(log(data$Price))
plot(price.log)

#Living_space
living.log <- density(log(data$Living_space))
plot(living.log)

#Rooms
rooms.log <- log(data$Rooms)
hist(rooms.log)

#Lot
lot.log <- density(log(data$Lot))
plot(lot.log)
```


# WEEK 1

## Linear Models

**Simple linear regression**


the variables Price, Living_space and Rooms are checked for "na" values
```{r, message=F, error=F}
options(scipen=999) #block scientific notation
library(ggplot2)
attach(data)
any(is.na(Price))
any(is.na(Living_space))
any(is.na(Rooms))
```

Data Visualisation

We plot the response variable "Price" against the predictor "Living_Space" to get a first impression and grahical analysis.

```{r, message=F}
#Living_space
ggplot(data, aes(log(Living_space), log(Price))) + geom_point() + geom_smooth(method = lm, se = T, color = 'red')

lm.log.price_living <- lm(log(Price) ~ log(Living_space))
summary(lm.log.price_living)
```
It seems to be a positive relationship between these two variables. More livingspace seems to have a higher price. So we fit a simple regression model to the data.

Before we interpret the intercept and the second coefficient, the slope, we exponentiate the values. The results are: For the intercept exp(8.17522)= 3551.84 and the slope exp(0.9028)=2.47

Therefore the interpretation would be: With a livingspace of 0 the price would be 3551.84 EURO and with each unit increase of the livingspace the price increase by 2.47 EURO which does not make much sense.

Anyway the p-value is very small therefore we have a string evidence that the slope for livingspace is not flat.

In an next step we examine the data set graphically and consider again "Price" as response variable but as predictor "Rooms" and we fit the model again with a simple linear regression.

```{r,message=F}
#Rooms
ggplot(data, aes(Rooms,Price)) + geom_point()+ geom_smooth(method = lm, se = F, color= 'red') +
scale_x_continuous(breaks = rep(1:20,len=20))

lm.log.price_rooms <- lm(log(Price) ~ log(Rooms))
summary(lm.log.price_rooms)
```

Now we are testing the effect of the categorical variable "Type" and visualize them first with a boxplot with and one without outliers.
```{r}
types.box.with_outlier <- ggplot(data, aes(x=Type, y=log(Price))) + geom_boxplot(outlier.colour = 'red')+ theme(axis.text.x = element_text(angle = 90)) + ggtitle('Type, Price with outliers in red')
plot(types.box.with_outlier)

types.box.no_outlier <- ggplot(data, aes(x=Type, y=log(Price))) + geom_boxplot(outlier.shape = NA)+ theme(axis.text.x = element_text(angle = 90)) + ggtitle('Type, Price without outliers')
plot(types.box.no_outlier)

lm.type <- lm(log(Price) ~ Type)
summary(lm.type)


```
Testing categorical variable (Furnishing_quality)and comparing by F-test 
```{r}
types.box.with_outlier <- ggplot(data, aes(x=Furnishing_quality, y=log(Price))) + geom_boxplot(outlier.colour = 'red')+ theme(axis.text.x = element_text(angle = 90)) + ggtitle('Furnishing Quality, Price with outliers in red')
plot(types.box.with_outlier)

types.box.no_outlier <- ggplot(data, aes(x=Furnishing_quality, y=log(Price))) + geom_boxplot(outlier.shape = NA)+ theme(axis.text.x = element_text(angle = 90)) + ggtitle('Furnishing Quality, Price without outliers')
plot(types.box.no_outlier)

lm.furnishing <- lm(log(Price) ~ Furnishing_quality)
summary(lm.furnishing)

lm.furnishing1 <- lm(log(Price) ~ 1)
summary(lm.furnishing1)

anova(lm.furnishing1, lm.furnishing)

```

Adding more categorical variables to the testing above --> Multiple linear regression?
```{r}

lm.furnishing2 <- update(lm.furnishing,. ~ . + Type + Condition)
formula(lm.furnishing2)
drop1(lm.furnishing2, test = "F")

```



# WEEK 2

## model nonlinearity


Polynomials
Graphical analysis
```{r, message = F, error = F}
library(mgcv)
attach(data)
library(ggplot2)
``` 

```{r, message = F}
#log(Price) ~ Rooms
gg.PriceRooms <- ggplot(data,
mapping = aes(y = log(Price),
x = Rooms)) +
geom_point()
gg.PriceRooms +
geom_smooth() +
scale_x_continuous(breaks = rep(1:20,len=20))


gam.1 <- gam(log(Price) ~ s(Rooms))
summary(gam.1)


plot(gam.1, residuals = TRUE, cex = 2)
```

log(Prices) ~ log(Rooms)
```{r, message = F}
#log(Rooms)
gg.PriceRooms <- ggplot(data,
mapping = aes(y = log(Price),
x = log(Rooms))) +
geom_point()
gg.PriceRooms +
geom_smooth() 

gam.2 <- gam(log(Price) ~ s(Rooms))
summary(gam.2)
plot(gam.2, residuals = TRUE, cex = 2)


gg.PriceLiving <- ggplot(data,
mapping = aes(y = log(Price),
x = log(Living_space))) +
geom_point()
gg.PriceLiving +
geom_smooth()

gam.3 <- gam(log(Price) ~ log(Living_space) +
s(Rooms) + s(Garages),
data = data)
summary(gam.3)
plot(gam.3, residuals = TRUE, select = 1)
```


```{r, message = F}
gg.LotPrice <- ggplot(data = data,
mapping = aes(y = log(Lot),
x = log(Price))) +
geom_point()
gg.LotPrice +
geom_smooth(method = 'gam')
```


# WEEK 3

## GLM

**Possion Model**

```{r}
glm.rooms <- glm(Rooms ~ Type,
family = "poisson",
data = data)

summary(glm.rooms)

```

```{r}
set.seed(99)
sim.data.rooms.Poisson <- simulate(glm.rooms)
##
NROW(sim.data.rooms.Poisson)

head(sim.data.rooms.Poisson)
tail(sim.data.rooms.Poisson)

```
Visualisierung ;)
``` {r}
ggplot(mapping = aes(y = sim.data.rooms.Poisson$sim_1,
x = data$Type)) +
geom_boxplot() +
geom_hline(yintercept = 0) +
ylab("simulated no. of rooms\n(assuming Poisson dist)") +
xlab("type")
```



``` {r}


#install.packages('mltools')
library(mltools)

# Resulting bins have an equal number of observations in each group
data[, "wt2"] <- bin_data(data$Price, bins=4, binType = "quantile")

# Resulting bins are equally spaced from min to max
data[, "wt3"] <- bin_data(data$Price, bins=4, binType = "explicit")

# Or if you'd rather define the bins yourself
data[, "wt4"] <- bin_data(data$Price, bins=c(-Inf, 250, 322, Inf), binType = "explicit")
head(data)
```

``` {r}
glm.roomswt2 <- glm(Rooms ~ wt2,
family = "poisson",
data = data)

summary(glm.roomswt2)
```
``` {r}
set.seed(99)
sim.data.rooms.Poissonwt2 <- simulate(glm.roomswt2)
##
NROW(sim.data.rooms.Poissonwt2)

head(sim.data.rooms.Poissonwt2)
tail(sim.data.rooms.Poissonwt2)
```
``` {r}
library(ggplot2)
ggplot(mapping = aes(y = sim.data.rooms.Poissonwt2$sim_1,
x = data$wt2)) +
geom_boxplot() +
geom_hline(yintercept = 0) +
ylab("simulated no. of rooms\n(assuming Poisson dist)") +
xlab("Groups")
```
Binary Model
Let’s fit a binary model

``` {r}
data$MillionYes <- ifelse(data$Price > 1000000, 1, 0)
data$MillionYes

ggplot(data = data,
mapping = aes(y = MillionYes,
x = log(Living_space))) +
geom_point()
```
Let’s fit a logistic regression model and add fit to the this graph

``` {r}
ggplot(data = data,
       mapping = aes(y = MillionYes,
                     x = log(Living_space))) + 
  geom_point() +
  geom_smooth(method = "glm", 
              se = FALSE,
              method.args = list(family = "binomial")) 
```


# WEEK 5

## ANN

**neuralnet package**

preparing data for neuralnet
```{r}
library(tidyverse)
library(data.table)
library(neuralnet)
library(caret)
```

```{r}
str(data)

df <- data.frame(data$Price, data$Living_space, data$Lot, data$Rooms, data$Year_built)

df.dropna <- na.omit(df)
df.house <- data.table(df.dropna)

any(is.na(df.house))
str(df.house)


```

**Prepare for Training**

```{r}
set.seed(123)
indices <- createDataPartition(df.house$data.Price, p = 0.8, list = FALSE)
train <- df.house %>% slice(indices)
test <- df.house %>% slice(-indices)
boxplot(train$data.Price, test$data.Price, df.house %>% sample_frac(0.2) %>% pull(data.Price))
```



```{r}
max <- apply(df.house, 2, max)
min <- apply(df.house, 2, min)
df.house_scaled <- as.data.frame(scale(df.house, center = min, scale = max - min))
train_scaled <- df.house_scaled %>% slice(indices)
test_scaled <- df.house_scaled %>% slice(-indices)
```


## Fit the Network

```{r}
set.seed(42)
housing_net = neuralnet(data.Price ~ data.Living_space + data.Lot + data.Rooms + data.Year_built, train_scaled, hidden = 3 , linear.output = TRUE)
plot(housing_net)
```



```{r}
pred_scaled <- compute(housing_net, test_scaled %>% select(-data.Price))
pred <- pred_scaled$net.result * (max(df.house$data.Price) - min(df.house$data.Price)) + min(df.house$data.Price)
#pred
```

```{r}
plot(test$data.Price, pred, col='blue', pch=16, ylab = "predicted Price NN", xlab = "real Price")
abline(0,1)
```
And calculate the RMSE
```{r}
sqrt(mean((test$data.Price - pred)^2))
```

Let's try something else











Let's try something else



















